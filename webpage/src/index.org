#+TITLE: Adversarial Texts with Gradient Methods
#+OPTIONS: toc:nil
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>

* Abstract                                                           :ignore:
:PROPERTIES:
:CUSTOM_ID: h1-abstract-8d3ca
:END:

#+BEGIN_abstract
This page shows the complete result of our simple adversarial text generation
method.  You could find the source code and report here
[[https://github.com/gongzhitaao/adversarial-text][github/gongzhitaao/adversarial-text]].  Due to some reasons, the paper is
withdrawn from arXiV.  It is only available on GitHub.
#+END_abstract

* TOC                                                                :ignore:
:PROPERTIES:
:CUSTOM_ID: h1-toc-72578
:END:

#+TOC: headlines 2

* Introduction
:PROPERTIES:
:CUSTOM_ID: h1-introduction-238f4
:END:

It has been shown in cite:szegedy2013-intriguing that we can apply very subtle
noise to images to trick deep learning into wrong prediction with very high
confident.  One example in shown in Figure [[fig:imgdemo]].  A set of adversarial
images via different attacking algorithms are generated from a random image from
MNIST dataset.  The upper image in /Clean/ column is the original clean image.
The upper images in the following columns are adversarial images generated by
the corresponding attacking algorithm, based on the first clean image,
respectively.  The lower image in each column is the differece between the
adversarial image and the clean image, illustrated in heatmap.  Below each
column is the label predicted by the target model, along with probability in
parenthesis.  The algorithms demonstrated are
FGSM cite:goodfellow2014-explaining, FGVM cite:miyato2015-distributional,
JSMA cite:papernot2015-limitations and
DeepFool cite:moosavi-dezfooli2015-deepfool.  These are all gradient attacking
methods.

#+ATTR_HTML: :width 70%
#+CAPTION: Adversarial examples created on MNIST
#+NAME: fig:imgdemo
[[file:img/imgdemo.png]]

* Method
:PROPERTIES:
:CUSTOM_ID: h1-method-157a9
:END:

Our method is straightforward.  We find the adversarial text in the /embedding
space/, e.g. cite:pennington2014-glove.  Part of the difficulty of generating
adversarial texts is the discreteness of the input space.  Instead of working in
the raw input space, we first use the above method to find /candidate
adversarial embeddings/ in the embedding space, and use /nearest neighbor/ to
find the corresponding adversarial tokens.  Since the embedding space is
continuous, finding adversarials is the same as images.  Similar to images, most
of the time we only need to modify a couple tokens (out of hundreds) to change
to fool different text models.

The downside of this method is that we do not have control over the quality
(e.g., syntax, semantics) of the generated sentences.

* Result
:PROPERTIES:
:CUSTOM_ID: sec:result
:END:

Due to the size of the dataset, we only show 100 samples for each parameter
setting on the website.  The rest samples could be downloaded.  Note that some
tokens in the clean dataset are different from the original piece of text, since
these texts are also reconstructed by approximate nearest neighbor search for
convenience.  These does not affect the embedded vectors.

** Fast Gradient Sign Method (FGSM)
:PROPERTIES:
:CUSTOM_ID: subsec:fgsm
:END:

This was proposed in cite:goodfellow2014-explaining.  The adversarial noise is
computed as \(z = \epsilon \text{sign}\nabla L\).

#+CAPTION: Adversarial text via FGSM
#+NAME: tab:textadv-fgsm
| \epsilon  | 0.40            | 0.35            | 0.30            | 0.25            |
|-----------+-----------------+-----------------+-----------------+-----------------|
| IMDB      | [[file:result/imdb_fgsm_eps0.40.html][0.1213 / 0.1334]] | [[file:result/imdb_fgsm_eps0.35.html][0.1213 / 0.1990]] | [[file:result/imdb_fgsm_eps0.30.html][0.1213 / 0.4074]] | [[file:result/imdb_fgsm_eps0.25.html][0.1213 / 0.6770]] |
| Reuters-2 | [[file:result/reuters2_fgsm_eps0.40.html][0.0146 / 0.6495]] | [[file:result/reuters2_fgsm_eps0.35.html][0.0146 / 0.7928]] | [[file:result/reuters2_fgsm_eps0.30.html][0.0146 / 0.9110]] | [[file:result/reuters2_fgsm_eps0.25.html][0.0146 / 0.9680]] |
| Reuters-5 | [[file:result/reuters5_fgsm_eps0.40.html][0.1128 / 0.5880]] | [[file:result/reuters5_fgsm_eps0.35.html][0.1128 / 0.7162]] | [[file:result/reuters5_fgsm_eps0.30.html][0.1128 / 0.7949]] | [[file:result/reuters5_fgsm_eps0.25.html][0.1128 / 0.8462]] |

** Fast Gradient Value Method (FGVM)
:PROPERTIES:
:CUSTOM_ID: subsec:fgvm
:END:

This is a variant of FGSM, instead of gradients, FGVM uses the gradients
directly.  The noise is \(z = \epsilon\frac{\nabla L}{\|\nabla L\|_2}\).

#+CAPTION: Adversarial text via FGVM
#+NAME: tab:textadv-fgvm
| \epsilon  | 15              | 30              | 50              | 100             |
|-----------+-----------------+-----------------+-----------------+-----------------|
| IMDB      | [[file:result/imdb_fgvm_eps15.html][0.6888 / 0.8538]] | [[file:result/imdb_fgvm_eps30.html][0.6549 / 0.8354]] | [[file:result/imdb_fgvm_eps50.html][0.6277 / 0.8207]] | [[file:result/imdb_fgvm_eps100.html][0.5925 / 0.7964]] |
| Reuters-2 | [[file:result/reuters2_fgvm_eps15.html][0.7747 / 0.7990]] | [[file:result/reuters2_fgvm_eps30.html][0.7337 / 0.7538]] | [[file:result/reuters2_fgvm_eps50.html][0.6975 / 0.7156]] | [[file:result/reuters2_fgvm_eps100.html][0.6349 / 0.6523]] |
| Reuters-5 | [[file:result/reuters5_fgvm_eps15.html][0.5915 / 0.7983]] | [[file:result/reuters5_fgvm_eps30.html][0.5368 / 0.6872]] | [[file:result/reuters5_fgvm_eps50.html][0.4786 / 0.6085]] | [[file:result/reuters5_fgvm_eps100.html][0.4000 / 0.5111]] |

** DeepFool
:PROPERTIES:
:CUSTOM_ID: subsec:deepfool
:END:

This method is proposed in cite:moosavi-dezfooli2015-deepfool.  DeepFool
iteratively finds the optimal direction in which we need to /travel/ the minimum
distance to cross the decision boundary of the target model.  Although in
non-linear cases, this optimality is not guaranteed, DeepFool works well in
practice, and usually generates very subtle noise.  In many of the examples,
DeepFool alters the label of the text piece by replace only one word.

#+CAPTION: Adversarial text via DeepFool
#+NAME: tab:textadv-deepfool
| \epsilon  | 20              | 30              | 40              | 50              |
|-----------+-----------------+-----------------+-----------------+-----------------|
| IMDB      | [[file:result/imdb_deepfool_eps20.html][0.5569 / 0.8298]] | [[file:result/imdb_deepfool_eps30.html][0.5508 / 0.7225]] | [[file:result/imdb_deepfool_eps40.html][0.5472 / 0.6678]] | [[file:result/imdb_deepfool_eps50.html][0.5453 / 0.6416]] |
| Reuters-2 | [[file:result/reuters2_deepfool_eps20.html][0.4416 / 0.6766]] | [[file:result/reuters2_deepfool_eps30.html][0.4416 / 0.5236]] | [[file:result/reuters2_deepfool_eps40.html][0.4416 / 0.4910]] | [[file:result/reuters2_deepfool_eps50.html][0.4416 / 0.4715]] |
| Reuters-5 | [[file:result/reuters5_deepfool_eps20.html][0.1163 / 0.4034]] | [[file:result/reuters5_deepfool_eps30.html][0.1162 / 0.2222]] | [[file:result/reuters5_deepfool_eps40.html][0.1162 / 0.1641]] | [[file:result/reuters5_deepfool_eps50.html][0.1162 / 0.1402]] |

* Reference                                                          :ignore:
:PROPERTIES:
:CUSTOM_ID: h1-reference-fb6ad
:END:

#+BIBLIOGRAPHY: /home/gongzhitaao/.local/data/bibliography/refdb.bib plain limit:t option:--no-keywords
